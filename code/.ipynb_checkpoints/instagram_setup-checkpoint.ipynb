{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a702b3df",
   "metadata": {},
   "source": [
    "# Instagram_setup\n",
    "\n",
    "This notebook has reusable functions and shared resources for text preprocessing, DTM creation, \n",
    "and branded hashtag sentiment analysis for Instagram beauty brand data.\n",
    "\n",
    "Contents:\n",
    "- custom_words_toad: Domain-specific stopword list\n",
    "- preprocess(): Cleans and tokenizes captions\n",
    "- create_dtm(): Converts cleaned text to document-term matrix with metadata\n",
    "- compare_branded_hashtags(): Compares average sentiment of branded vs non-branded posts per hashtag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafac173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3b8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words_toad = [\n",
    "    # Brand names (removed from analysis)\n",
    "    'estee', 'lauder', 'tarte', 'fenty', 'glossier', 'cosrx', 'etude',\n",
    "    'sulwhasoo', 'laneige', 'innisfree', 'elf',\n",
    "\n",
    "    # Platform-related\n",
    "    'video', 'youtube', 'tiktok', 'instagram', 'reel', 'feed',\n",
    "    'post', 'stories', 'caption', 'social', 'media',\n",
    "\n",
    "    # Engagement / action words\n",
    "    'like', 'likes', 'comment', 'comments', 'share', 'save', 'follow', 'subscribe',\n",
    "    'tag', 'click', 'link', 'bio', 'visit', 'dm', 'available', 'check',\n",
    "\n",
    "    # Time / filler\n",
    "    'today', 'now', 'new', 'soon', 'launch', 'launching', 'stay', 'tune', 'coming', 'back',\n",
    "\n",
    "    # General beauty-related terms\n",
    "    'beauty', 'skin', 'skincare', 'routine', 'makeup', 'product', 'products',\n",
    "    'face', 'body', 'glow', 'look', 'formula', 'texture', 'result',\n",
    "\n",
    "    # Emoji / symbols\n",
    "    'âœ¨', 'ðŸ”¥', 'ðŸ’§', 'ðŸ’«', 'ðŸ˜', 'ðŸ’–', 'ðŸŒŸ', 'ðŸ’¥', 'ðŸ§´', 'ðŸ“¦', 'ðŸ›ï¸',\n",
    "\n",
    "    # Overused positive adjectives\n",
    "    'feel', 'love', 'use', 'try', 'amazing', 'favorite', 'best', 'perfect', 'must', 'obsessed',\n",
    "\n",
    "    # Promotional terms\n",
    "    'shop', 'buy', 'discount', 'deal', 'sale', 'off', 'gift', 'giveaway', 'free', 'offer',\n",
    "\n",
    "    # Conversation filler\n",
    "    'hey', 'hello', 'welcome', 'thank', 'you', 'everyone', 'guys', 'hi', 'omg', 'pls', 'yay', 'get', 'got', 'let', 'us'\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(df_col, custom_words_toad):\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Compile full custom stopword list\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    new_stopwords = set(list_stopwords + custom_words_toad)\n",
    "\n",
    "    corpus_lower = df_col.fillna(\"\").str.lower().to_list()\n",
    "\n",
    "    nostop_listing = []\n",
    "    for text in corpus_lower:\n",
    "        # Clean URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = [\n",
    "            word for word in wordpunct_tokenize(text)\n",
    "            if word.isalpha() and word not in new_stopwords\n",
    "        ]\n",
    "        # Apply stemming\n",
    "        stemmed_tokens = [porter.stem(word) for word in tokens if len(word) > 2]\n",
    "        nostop_listing.append(stemmed_tokens)\n",
    "\n",
    "    return nostop_listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6b310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function provided\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    \"\"\" \n",
    "    Function to create dense document-term matrix (DTM) from a list of strings and provided metadata. \n",
    "    A sparse DTM is a list of term_index/doc_index tuples: if a given term occurs in a given doc at least once, \n",
    "        then this count is listed as a tuple; if not, that term/doc pair is omitted. \n",
    "    In a dense DTM, each row is one text (e.g., an Airbnb listing), each column is a term, and \n",
    "        each cell indicates the frequency of that word in that text. \n",
    "    \n",
    "    Parameters:\n",
    "        list_of_strings (Series): each row contains a preprocessed string (need not be tokenized)\n",
    "        metadata (DataFrame): contains document-level covariates\n",
    "    \n",
    "    Returns:\n",
    "        Dense DTM with metadata on left and then one column per word in lexicon\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize a sklearn tokenizer; this helps us tokenize the preprocessed string input\n",
    "    vectorizer = CountVectorizer(lowercase = True, max_features=5000, min_df=5,           # ignore rare words\n",
    "        stop_words='english')  # or try 10000 if you can afford more memory) \n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    print('Sparse matrix form:\\n', dtm_sparse[:3]) # take a look at sparse representation\n",
    "    print()\n",
    "    \n",
    "    # switch the dataframe from the sparse representation to the normal dense representation (so we can treat it as regular dataframe)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out ())\n",
    "    print('Dense matrix form:\\n', dtm_dense_named.head()) # take a look at dense representation\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(drop=True), dtm_dense_named], axis = 1) # add back document-level covariates\n",
    "\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1f4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_branded_hashtags(df, brand_name):\n",
    "    \"\"\"\n",
    "    Compare average sentiment of branded vs. non-branded posts for each hashtag.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Dataframe with 'brand', 'hashtags', 'compound', and 'is_branded_content'\n",
    "        brand_name (str): The brand to filter on\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Pivot table of hashtags with sentiment comparison\n",
    "    \"\"\"\n",
    "    filtered = df[df[\"brand\"] == brand_name]\n",
    "\n",
    "    sentiment = (\n",
    "        filtered.groupby([\"hashtags\", \"is_branded_content\"])\n",
    "        .agg(avg_sentiment=(\"compound\", \"mean\"), count=(\"hashtags\", \"count\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    pivot = sentiment.pivot_table(\n",
    "        index=\"hashtags\",\n",
    "        columns=\"is_branded_content\",\n",
    "        values=\"avg_sentiment\"\n",
    "    )\n",
    "\n",
    "    pivot = pivot.rename(columns={\n",
    "        True: \"branded_sentiment\",\n",
    "        False: \"non_branded_sentiment\"\n",
    "    })\n",
    "\n",
    "    pivot[\"diff\"] = pivot.get(\"branded_sentiment\", 0) - pivot.get(\"non_branded_sentiment\", 0)\n",
    "\n",
    "    return pivot.dropna().sort_values(\"diff\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0949b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
