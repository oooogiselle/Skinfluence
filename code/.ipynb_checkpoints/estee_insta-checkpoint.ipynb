{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff496700",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "#!pip install nltk # can install on terminal or by uncommenting this line\n",
    "#import nltk; nltk.download('punkt'); nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## lda\n",
    "#!pip install gensim # can install by uncommenting this line\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## visualizing LDA--likely need to install\n",
    "#!pip install pyLDAvis # can install by uncommenting this line\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "import string; punctlist = [char for char in string.punctuation] # list of english punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1d11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import ast\n",
    "import networkx as nx\n",
    "import urllib, json\n",
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be8db8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c15dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estee_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_estee.csv\")\n",
    "estee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a173846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words_toad = [\n",
    "    'estee', 'lauder', 'esteelauder', 'esteelaudersg', 'advancednightrepair', 'anr',  \n",
    "    'video', 'youtube', 'tiktok', 'instagram', 'reel', 'feed',                        \n",
    "    'like', 'likes', 'comment', 'comments', 'share', 'save', 'follow', 'subscribe',  \n",
    "    'today', 'now', 'new', 'shop', 'buy', 'link', 'bio', 'visit', 'available',       \n",
    "    'beauty', 'skin', 'skincare', 'routine', 'makeup', 'product', 'products',       \n",
    "    '✨', '🔥', '💧', '💫', '😍', '💖',                                                \n",
    "    'feel', 'love', 'use', 'try', 'amazing', 'favorite', 'best',                     \n",
    "    'night', 'repair', 'serum', 'hydrating', 'hydration', 'cream',                    \n",
    "    'hey', 'hello', 'welcome', 'thank', 'you', 'everyone', 'guys'                    \n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(df_col, custom_words_toad):\n",
    "    porter = PorterStemmer()\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    new_stopwords = set(list_stopwords + custom_words_toad)\n",
    "\n",
    "    corpus_lower = df_col.fillna(\"\").str.lower().to_list()\n",
    "\n",
    "    nostop_listing = []\n",
    "    for text in corpus_lower:\n",
    "        # Clean URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = [\n",
    "            word for word in wordpunct_tokenize(text)\n",
    "            if word.isalpha() and word not in new_stopwords\n",
    "        ]\n",
    "        # Apply stemming\n",
    "        stemmed_tokens = [porter.stem(word) for word in tokens if len(word) > 2]\n",
    "        nostop_listing.append(stemmed_tokens)\n",
    "\n",
    "    return nostop_listing\n",
    "    \n",
    "estee_df[\"text_clean\"] = preprocess(estee_df[\"text\"], custom_words_toad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ba16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function provided\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    \"\"\" \n",
    "    Function to create dense document-term matrix (DTM) from a list of strings and provided metadata. \n",
    "    A sparse DTM is a list of term_index/doc_index tuples: if a given term occurs in a given doc at least once, \n",
    "        then this count is listed as a tuple; if not, that term/doc pair is omitted. \n",
    "    In a dense DTM, each row is one text (e.g., an Airbnb listing), each column is a term, and \n",
    "        each cell indicates the frequency of that word in that text. \n",
    "    \n",
    "    Parameters:\n",
    "        list_of_strings (Series): each row contains a preprocessed string (need not be tokenized)\n",
    "        metadata (DataFrame): contains document-level covariates\n",
    "    \n",
    "    Returns:\n",
    "        Dense DTM with metadata on left and then one column per word in lexicon\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize a sklearn tokenizer; this helps us tokenize the preprocessed string input\n",
    "    vectorizer = CountVectorizer(lowercase = True) \n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    print('Sparse matrix form:\\n', dtm_sparse[:3]) # take a look at sparse representation\n",
    "    print()\n",
    "    \n",
    "    # switch the dataframe from the sparse representation to the normal dense representation (so we can treat it as regular dataframe)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out ())\n",
    "    print('Dense matrix form:\\n', dtm_dense_named.head()) # take a look at dense representation\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(drop=True), dtm_dense_named], axis = 1) # add back document-level covariates\n",
    "\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b24a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process text more, lower and to string\n",
    "estee_df[\"text_clean_str\"] = estee_df[\"text_clean\"].apply(lambda tokens: \" \".join(tokens).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be60c3",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a611734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract GPEs from one string\n",
    "def get_org(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adeb2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract product from one string\n",
    "def get_product(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PRODUCT\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ea7e7",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba17a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0c7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estee_df[\"sentiment\"] = estee_df[\"text_clean_str\"].apply(sent_obj.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6174703",
   "metadata": {},
   "outputs": [],
   "source": [
    "estee_df[\"compound\"] = estee_df[\"sentiment\"].apply(lambda x: x[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83cd7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x = estee_df[\"statistics.views\"], y = estee_df.compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a267a",
   "metadata": {},
   "source": [
    "This scatterplot shows the relationship between:\n",
    "- **x-axis**: statistics.views, which is the number of views an Instagram post got\n",
    "- **y-axis**: compound, which is the sentiment score from VADER (+1 = very positive, -1 = very negative)\n",
    "\n",
    "As we can see, most of the estee lauder posts have a view count of under 2 millions views, with most posts having a **positive sentiment**. The regression line is slightly rising, meaning posts with more views tend to have slightly more positive sentiment, but the effect is very small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ddb95",
   "metadata": {},
   "source": [
    "### Sentiment Analysis for Branded vs Nonbraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6719819",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing Branded and NonBranded Sentiment\n",
    "sns.lmplot(data=estee_df, x=\"statistics.views\", y=\"compound\", hue=\"is_branded_content\", scatter_kws={\"alpha\": 0.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f6fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_brand_num = len(estee_df[estee_df[\"is_branded_content\"] == True])\n",
    "is_brand_num\n",
    "nonbrand_num = len(estee_df[estee_df[\"is_branded_content\"] == False])\n",
    "nonbrand_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf17c5",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The plot shows a clear imbalance in the dataset: there are far more non-branded posts than branded ones (16,988 vs. 401). Most branded posts cluster around high sentiment scores, especially between 0.75 and 1.0, indicating strong positive sentiment. This aligns with expectations—branded content tends to use more promotional and upbeat language.\n",
    "\n",
    "In contrast, non-branded content spans a broader sentiment range, including both highly positive and negative values, suggesting more varied and authentic user expression.\n",
    "\n",
    "Interestingly, while branded content is consistently positive, it doesn’t appear to drive significantly higher view counts—both branded and non-branded posts are heavily concentrated under 1 million views. The weak slope of the regression line also indicates that sentiment is not a strong predictor of view count. This suggests that while branded posts may aim to maintain positivity, it’s not necessarily sentiment that drives engagement or visibility. Overall, branded content appears polished and emotionally safe, whereas non-branded content provides richer insights into public perception, making it valuable for understanding audience sentiment in a more nuanced way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0428a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing Branded and NonBranded Sentiment and like count\n",
    "sns.lmplot(data=estee_df, x=\"statistics.like_count\", y=\"compound\", hue=\"is_branded_content\", scatter_kws={\"alpha\": 0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bfea2",
   "metadata": {},
   "source": [
    "From the plot, we can see a distinct difference in how sentiment relates to like counts for branded and non-branded content.\n",
    "\n",
    "- **Branded posts (orange)** tend to cluster in the **positive** sentiment range, especially between compound scores of 0.5 to 1.0. The slight **upward** trend suggests that more liked branded content tends to be more positive — though the sample size may be small.\n",
    "\n",
    "- **Non-branded posts (blue)** show a wider spread of sentiment, ranging from very negative to very positive. Interestingly, there's a **slight negative** trend: more liked non-branded posts actually correlate with slightly lower sentiment. This might reflect how critical or edgy posts can still attract engagement. That said, this trend might be misleading --- there is only one non-branded post with an unusually high like count and low sentiment, which could skew the regression line. Most content overall stays within a lower like range regardless of tone.\n",
    "\n",
    "The concentration near the origin (low like counts) for both categories suggests that most posts don't go viral — and sentiment doesn't strongly predict popularity in the general case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42b97b",
   "metadata": {},
   "source": [
    "### Hashtags and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8882ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to list \n",
    "estee_df['hashtags'] = estee_df['hashtags'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    ")\n",
    "hashtag_df = estee_df.explode(\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d158bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab76ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_sentiment = (\n",
    "    hashtag_df.groupby(\"hashtags\")[\"compound\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dc5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_stats = (\n",
    "    hashtag_df.groupby(\"hashtags\")\n",
    "    .agg(avg_sentiment=(\"compound\", \"mean\"), count=(\"compound\", \"count\"))\n",
    "    .sort_values(by=\"avg_sentiment\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84629a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = hashtag_stats[hashtag_stats[\"count\"] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a20341a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = filtered.head(10)\n",
    "bottom10 = filtered.tail(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "top10[\"avg_sentiment\"].plot(kind=\"barh\", color=\"green\", title=\"Top Hashtags by Avg Sentiment\")\n",
    "plt.xlabel(\"Avg Compound Sentiment\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bottom10[\"avg_sentiment\"].plot(kind=\"barh\", color=\"red\", title=\"Lowest Sentiment Hashtags\")\n",
    "plt.xlabel(\"Avg Compound Sentiment\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2b44c",
   "metadata": {},
   "source": [
    "### Hashtag Sentiment Analysis Interpretation\n",
    "#### Top Hashtags by Sentiment Analysis\n",
    "Based on the two graphs, the hashtags with the highest average sentiment include **#femalepreneur, #makeupgiveaway, and #multieffecteyecream**, indicating strong positive reactions from users. This suggests that Estée Lauder’s campaigns promoting **women’s empowerment, product giveaways, and skincare lines** are resonating well with audiences. Additionally, high sentiment associated with hashtags in*Bahasa Indonesia or Malay (e.g., #jualesteelaudertermurah) indicates that Estée Lauder enjoys positive brand perception in **Southeast Asian** markets, especially in the context of affordability and accessibility. Overall, the consistently positive sentiment across these hashtags suggests that the brand’s marketing efforts are both emotionally resonant and internationally effective.\n",
    "\n",
    "#### Lowest Hashtags by Sentiment Analysis\n",
    "The graph displays the hashtags with the lowest average sentiment scores, with terms like **#breastcancerawarenessmonth, #timetoeendbreastcancer, and #pinkribbon** appearing prominently. These hashtags are closely tied to Breast Cancer Awareness campaigns, yet their low sentiment scores likely reflect a **limitation** of the sentiment analysis tool rather than genuine negativity. Models like VADER rely on individual word polarity, so emotionally heavy terms such as \"cancer,\" \"diagnosed,\" or \"survivor\" can skew the sentiment negatively, even when the overall message is **hopeful, supportive, or awareness-driven**. For example, a post honoring a survivor or discussing the impact of breast cancer may contain compassionate intent but still be flagged as negative due to language associated with illness or loss. You can see the text and their compound rating below. Additionally, hashtags like #blackwomen may be part of broader conversations about health equity or underrepresentation, which can surface complex or critical discourse not easily captured by standard sentiment tools. These results highlight the importance of considering context and social intent when interpreting sentiment scores, especially in campaigns related to health, advocacy, and inclusion.\n",
    "\n",
    "In reality, these posts are often tied to deeply meaningful, supportive messaging. Their frequency in the dataset actually points to Estée Lauder’s **strong commitment to breast cancer advocacy**, showing that the brand continues to spotlight this cause prominently across their content.\n",
    "\n",
    "To better reflect the true tone of such socially driven campaigns, a more **nuanced** analysis would be needed—such as applying context-aware models (like transformer-based sentiment classifiers), adding a custom label for awareness content, or even conducting qualitative keyword and theme analysis rather than relying solely on sentiment scores. This would avoid misinterpreting emotionally sensitive but impactful content and give a more accurate picture of Estée Lauder’s brand voice and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "445ef876",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_df = estee_df[estee_df['hashtags'].astype(str).str.contains(\"breastcancer\", case=False, na=False)]\n",
    "text_compound = bc_df[[\"text\", \"compound\", \"sentiment\"]]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "text_compound.sample(10)  # 10 random rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba361c",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5cdddf",
   "metadata": {},
   "source": [
    "###  Focus on Branded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38b09c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "branded_df = estee_df[estee_df['is_branded_content'] == True]\n",
    "# Flatten all hashtags\n",
    "flat_tags = [tag for tags in branded_df['hashtags'] if isinstance(tags, list) for tag in tags]\n",
    "top_tags = set([tag for tag, _ in Counter(flat_tags).most_common(50)])  # or 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "509aa4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_edges = []\n",
    "for tags in elf_df[\"hashtags\"]:\n",
    "    if isinstance(tags, list):\n",
    "        tags = [tag.strip() for tag in tags if isinstance(tag, str) and tag.strip()]\n",
    "        if len(tags) > 1:\n",
    "            co_occurrence_edges.extend(combinations(sorted(set(tags)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f738d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_counts = Counter(co_occurrence_edges)\n",
    "edge_df = pd.DataFrame(edge_counts.items(), columns=[\"pair\", \"weight\"])\n",
    "edge_df[\"source\"] = edge_df[\"pair\"].apply(lambda x: x[0])\n",
    "edge_df[\"target\"] = edge_df[\"pair\"].apply(lambda x: x[1])\n",
    "edge_df = edge_df[[\"source\", \"target\", \"weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250891a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only strong co-occurrence edges\n",
    "filtered_edge_df = edge_df[edge_df[\"weight\"] >= 5]  # Try 3, 4, or even 5\n",
    "G = nx.from_pandas_edgelist(filtered_edge_df, source='source', target='target', edge_attr='weight')\n",
    "\n",
    "# Create 'group' attribute using group_map\n",
    "for node in G.nodes():\n",
    "    group = group_map.get(node, default_group)\n",
    "    G.nodes[node]['group'] = group\n",
    "    G.nodes[node]['color'] = group2color[group]  # Optional: for future coloring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9743d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "GG = G.subgraph(largest_cc).copy()\n",
    "\n",
    "stylized_network, config = visualize(GG, port=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network[:5]  # Show first few node entries to confirm group/color are applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d818bb",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9979b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example application on raw lowercase texts; \n",
    "dtm_nopre = create_dtm(list_of_strings= estee_df.text_clean_str,\n",
    "                      metadata = estee_df[['is_branded_content', 'hashtags', 'post_owner.name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa886b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show first set of rows/cols\n",
    "dtm_nopre.head()\n",
    "\n",
    "## show arbitrary later cols in resulting data\n",
    "dtm_nopre.shape\n",
    "dtm_nopre.iloc[0:5, 480:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b38034",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = dtm_nopre[dtm_nopre.columns[4:]].sum(axis = 0)\n",
    "\n",
    "## sorting from most frequent to least frequent\n",
    "top_terms.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm_nopre.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f82884",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: re-tokenize and store in list\n",
    "## here, i'm doing with the raw random sample of text\n",
    "## in activity, you should do with the preprocessed texts\n",
    "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
    "                  estee_df.text_clean_str]\n",
    "\n",
    "\n",
    "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
    "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
    "raw_len = len(text_raw_dict) # get length for comparison below\n",
    "\n",
    "### explore first few keys and values\n",
    "### see that key is just an arbitrary counter; value is the word itself\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
    "\n",
    "\n",
    "## Step 3: filter out very rare and very common words\n",
    "## here, i'm using the threshold that a word needs to appear in at least\n",
    "## 5% of docs but not more than 95%\n",
    "## this is an integer count of docs so i round\n",
    "lower_bound = round(estee_df.shape[0]*0.05)\n",
    "upper_bound = round(estee_df.shape[0]*0.95)\n",
    "\n",
    "### apply filtering to dictionary\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                             no_above = upper_bound)\n",
    "print(f'Filtering out very rare and very common words reduced the \\\n",
    "length of dictionary from {str(raw_len)} to {str(len(text_raw_dict))}.')\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]} # show first five entries after filtering\n",
    "\n",
    "## Step 4: apply dictionary to TOKENIZED texts\n",
    "## this creates a mapping between each word \n",
    "## in a specific listing and the key in the dictionary.\n",
    "## for words that remain in the filtered dictionary,\n",
    "## output is a list where len(list) == n documents\n",
    "## and each element in the list is a list of tuples\n",
    "## containing the mappings\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### can apply doc2bow(one_text, return_missing = True) to print words\n",
    "### eliminated from the listing bc they're not in filtered dictionary.\n",
    "### but feeding that one with missing values to\n",
    "### the lda function can cause errors\n",
    "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
    "                            for one_text in text_raw_tokens]\n",
    "print('Sample of documents represented in dictionary format (with omitted words noted):')\n",
    "corpus_fromdict_showmiss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: we're finally ready to estimate the model!\n",
    "## full documentation here - https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "## here, we're feeding the lda function:\n",
    "## (1) the corpus we created from the dictionary,\n",
    "## (2) a parameter we decide on for the number of topics (k),\n",
    "## (3) the dictionary itself,\n",
    "## (4) parameter for number of passes through training data (more means slower), and\n",
    "## (5) parameter that returns, for each word remaining in dict, the topic probabilities.\n",
    "## see documentation for many other arguments you can vary\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                         num_topics = 5, \n",
    "                                         id2word=text_raw_dict, \n",
    "                                         passes=6, \n",
    "                                         alpha = 'auto',\n",
    "                                         per_word_topics = True)\n",
    "\n",
    "print(type(ldamod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c902a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-model 1: explore corpus-wide summary of topics\n",
    "### getting the topics and top words; can retrieve diff top words\n",
    "topics = ldamod.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2603a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "## Post-model 2: explore topics associated with each document\n",
    "### for each item in our original dictionary, get list of topic probabilities\n",
    "l=[ldamod.get_document_topics(item) for item in corpus_fromdict]\n",
    "### print result\n",
    "text_raw_tokens[0:5]\n",
    "l[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = gensimvis.prepare(ldamod, corpus_fromdict, text_raw_dict)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamod.print_topics(num_words=10)\n",
    "for i, topic in topics:\n",
    "    print(f\"Topic {i}: {topic}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    0: \"Makeup\",\n",
    "    1: \"Gifts & Retail\",\n",
    "    2: \"Skincare\",\n",
    "    3: \"Campaign\",\n",
    "    4: \"Work\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e0819",
   "metadata": {},
   "source": [
    "## LDA Conclusion\n",
    "Our LDA model surfaced five main themes in Estée Lauder’s Instagram posts:\n",
    "\n",
    "- Makeup – Posts focused on product launches, tutorials, and beauty looks.\n",
    "\n",
    "- Gifts & Retail – Content around gift sets, holiday promos, and store campaigns.\n",
    "\n",
    "- Skincare – Posts highlighting skincare benefits, ingredients, and routines.\n",
    "\n",
    "- Campaign – Branded hashtags, slogans, and collabs with ambassadors.\n",
    "\n",
    "- Work – Behind-the-scenes moments and glimpses into the team or brand culture.\n",
    "\n",
    "Overall, Estée Lauder’s content blends product focus with brand storytelling and seasonal marketing, giving followers both inspiration and insight into the company.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bb23d",
   "metadata": {},
   "source": [
    "## Combine LDA and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70a104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
