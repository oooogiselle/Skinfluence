{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dae253",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "#!pip install nltk # can install on terminal or by uncommenting this line\n",
    "#import nltk; nltk.download('punkt'); nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## lda\n",
    "#!pip install gensim # can install by uncommenting this line\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## visualizing LDA--likely need to install\n",
    "#!pip install pyLDAvis # can install by uncommenting this line\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "import string; punctlist = [char for char in string.punctuation] # list of english punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2646a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import ast\n",
    "import networkx as nx\n",
    "import urllib, json\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from netwulf import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a213b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estee_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_estee.csv\")\n",
    "#tarte_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_tarte.csv\")\n",
    "#innisfree_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_innisfree.csv\")\n",
    "#elf_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_elf.csv\")\n",
    "'''glossier_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_glossier.csv\",\n",
    "    low_memory=False)\n",
    "laneige_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_laneige.csv\")\n",
    "sulwhasoo_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_sulwhasoo.csv\")\n",
    "etude_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_etude.csv\")\n",
    "cosrx_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_cosrx.csv\",low_memory=False)\n",
    "fenty_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/instagram_fenty.csv\",low_memory=False)'''\n",
    "all_df = pd.read_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/all_brands_cleaned.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09f664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''estee_df[\"brand\"] = \"EstÃ©e Lauder\"\n",
    "tarte_df[\"brand\"] = \"Tarte\"\n",
    "innisfree_df[\"brand\"] = \"Innisfree\"\n",
    "elf_df[\"brand\"] = \"e.l.f\"\n",
    "glossier_df[\"brand\"] = \"Glossier\"\n",
    "laneige_df[\"brand\"] = \"Laneige\"\n",
    "sulwhasoo_df[\"brand\"] = \"Sulwhasoo\"\n",
    "etude_df[\"brand\"] = \"Etude\"\n",
    "cosrx_df[\"brand\"] = \"COSRX\"\n",
    "fenty_df[\"brand\"] = \"Fenty Beauty\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a02ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat everything\n",
    "'''\n",
    "all_df = pd.concat([\n",
    "    estee_df, tarte_df, innisfree_df, elf_df, glossier_df,\n",
    "    laneige_df, sulwhasoo_df, etude_df, cosrx_df, fenty_df\n",
    "], ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13917543",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53936291",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words_toad = [\n",
    "    # Brand names (removed from analysis)\n",
    "    'estee', 'lauder', 'tarte', 'fenty', 'glossier', 'cosrx', 'etude',\n",
    "    'sulwhasoo', 'laneige', 'innisfree', 'elf',\n",
    "\n",
    "    # Platform-related\n",
    "    'video', 'youtube', 'tiktok', 'instagram', 'reel', 'feed',\n",
    "    'post', 'stories', 'caption', 'social', 'media',\n",
    "\n",
    "    # Engagement / action words\n",
    "    'like', 'likes', 'comment', 'comments', 'share', 'save', 'follow', 'subscribe',\n",
    "    'tag', 'click', 'link', 'bio', 'visit', 'dm', 'available', 'check',\n",
    "\n",
    "    # Time / filler\n",
    "    'today', 'now', 'new', 'soon', 'launch', 'launching', 'stay', 'tune', 'coming', 'back',\n",
    "\n",
    "    # General beauty-related terms\n",
    "    'beauty', 'skin', 'skincare', 'routine', 'makeup', 'product', 'products',\n",
    "    'face', 'body', 'glow', 'look', 'formula', 'texture', 'result',\n",
    "\n",
    "    # Emoji / symbols\n",
    "    'âœ¨', 'ðŸ”¥', 'ðŸ’§', 'ðŸ’«', 'ðŸ˜', 'ðŸ’–', 'ðŸŒŸ', 'ðŸ’¥', 'ðŸ§´', 'ðŸ“¦', 'ðŸ›ï¸',\n",
    "\n",
    "    # Overused positive adjectives\n",
    "    'feel', 'love', 'use', 'try', 'amazing', 'favorite', 'best', 'perfect', 'must', 'obsessed',\n",
    "\n",
    "    # Promotional terms\n",
    "    'shop', 'buy', 'discount', 'deal', 'sale', 'off', 'gift', 'giveaway', 'free', 'offer',\n",
    "\n",
    "    # Conversation filler\n",
    "    'hey', 'hello', 'welcome', 'thank', 'you', 'everyone', 'guys', 'hi', 'omg', 'pls', 'yay', 'get', 'got', 'let', 'us'\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(df_col, custom_words_toad):\n",
    "    porter = PorterStemmer()\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    new_stopwords = set(list_stopwords + custom_words_toad)\n",
    "\n",
    "    corpus_lower = df_col.fillna(\"\").str.lower().to_list()\n",
    "\n",
    "    nostop_listing = []\n",
    "    for text in corpus_lower:\n",
    "        # Clean URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = [\n",
    "            word for word in wordpunct_tokenize(text)\n",
    "            if word.isalpha() and word not in new_stopwords\n",
    "        ]\n",
    "        # Apply stemming\n",
    "        stemmed_tokens = [porter.stem(word) for word in tokens if len(word) > 2]\n",
    "        nostop_listing.append(stemmed_tokens)\n",
    "\n",
    "    return nostop_listing\n",
    "    \n",
    "# already ran this before\n",
    "#all_df[\"text_clean\"] = preprocess(all_df[\"text\"], custom_words_toad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f2c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function provided\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    \"\"\" \n",
    "    Function to create dense document-term matrix (DTM) from a list of strings and provided metadata. \n",
    "    A sparse DTM is a list of term_index/doc_index tuples: if a given term occurs in a given doc at least once, \n",
    "        then this count is listed as a tuple; if not, that term/doc pair is omitted. \n",
    "    In a dense DTM, each row is one text (e.g., an Airbnb listing), each column is a term, and \n",
    "        each cell indicates the frequency of that word in that text. \n",
    "    \n",
    "    Parameters:\n",
    "        list_of_strings (Series): each row contains a preprocessed string (need not be tokenized)\n",
    "        metadata (DataFrame): contains document-level covariates\n",
    "    \n",
    "    Returns:\n",
    "        Dense DTM with metadata on left and then one column per word in lexicon\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize a sklearn tokenizer; this helps us tokenize the preprocessed string input\n",
    "    vectorizer = CountVectorizer(lowercase = True, max_features=5000  # or try 10000 if you can afford more memory) \n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    print('Sparse matrix form:\\n', dtm_sparse[:3]) # take a look at sparse representation\n",
    "    print()\n",
    "    \n",
    "    # switch the dataframe from the sparse representation to the normal dense representation (so we can treat it as regular dataframe)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out ())\n",
    "    print('Dense matrix form:\\n', dtm_dense_named.head()) # take a look at dense representation\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(drop=True), dtm_dense_named], axis = 1) # add back document-level covariates\n",
    "\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574e836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process text more, lower and to string\n",
    "#all_df[\"text_clean_str\"] = all_df[\"text_clean\"].apply(lambda tokens: \" \".join(tokens).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbc1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df.to_csv(\"/Users/giselle/Desktop/Dartmouth/Skinfluence/data/all_brands_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed23187",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d5c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"sentiment\"] = all_df[\"text_clean_str\"].astype(str).apply(sent_obj.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec73b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"compound\"] = all_df[\"sentiment\"].apply(lambda x: x[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07250d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x = all_df[\"statistics.views\"], y = all_df.compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=all_df, x=\"brand\", y=\"compound\", palette=\"Set2\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Sentiment Distribution per Brand\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentiment = all_df.groupby(\"brand\")[\"compound\"].mean().sort_values()\n",
    "sns.barplot(x=avg_sentiment.values, y=avg_sentiment.index)\n",
    "plt.title(\"Average Sentiment Score by Brand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=all_df, x=\"statistics.like_count\", y=\"compound\", hue=\"is_branded_content\", scatter_kws={\"alpha\": 0.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to list \n",
    "all_df['hashtags'] = all_df['hashtags'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    ")\n",
    "hashtag_df = all_df.explode(\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2966e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize hashtag text\n",
    "hashtag_df[\"hashtags\"] = hashtag_df[\"hashtags\"].astype(str).str.lower().str.strip()\n",
    "hashtag_df = hashtag_df[hashtag_df[\"hashtags\"].notna() & (hashtag_df[\"hashtags\"] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48752f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sentiment per brand and hashtag\n",
    "hashtag_sentiment = (\n",
    "    hashtag_df.groupby([\"brand\", \"hashtags\"])\n",
    "    .agg(avg_sentiment=(\"compound\", \"mean\"), count=(\"hashtags\", \"count\"))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_stats = (\n",
    "    hashtag_df.groupby(\"hashtags\")\n",
    "    .agg(avg_sentiment=(\"compound\", \"mean\"), count=(\"compound\", \"count\"))\n",
    "    .sort_values(by=\"avg_sentiment\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9be1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = hashtag_stats[hashtag_stats[\"count\"] >= 50]\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each brand\n",
    "brands = hashtag_sentiment[\"brand\"].unique()\n",
    "hashtag_sentiment[\"hashtags\"] = hashtag_sentiment[\"hashtags\"].astype(str)\n",
    "hashtag_sentiment = hashtag_sentiment[\n",
    "    hashtag_sentiment[\"hashtags\"].apply(lambda x: all(char in string.printable for char in x))\n",
    "]\n",
    "\n",
    "for brand in brands:\n",
    "    # Filter sentiment data to current brand\n",
    "    filtered = hashtag_sentiment[hashtag_sentiment[\"brand\"] == brand]\n",
    "\n",
    "    # Apply frequency filter: only hashtags used at least 10 times\n",
    "    filtered = filtered[filtered[\"count\"] >= 10]\n",
    "\n",
    "    # Sort by average sentiment\n",
    "    filtered = filtered.sort_values(\"avg_sentiment\", ascending=False)\n",
    "\n",
    "    # Skip brands with fewer than 10 qualifying hashtags\n",
    "    if len(filtered) < 10:\n",
    "        continue\n",
    "\n",
    "    top10 = filtered.head(10)\n",
    "    bottom10 = filtered.tail(10)\n",
    "\n",
    "    # Top 10 Hashtags\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(top10[\"hashtags\"], top10[\"avg_sentiment\"], color=\"green\")\n",
    "    plt.title(f\"{brand}: Top 10 Hashtags by Average Sentiment (min 10 uses)\")\n",
    "    plt.xlabel(\"Avg Compound Sentiment\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Bottom 10 Hashtags\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(bottom10[\"hashtags\"], bottom10[\"avg_sentiment\"], color=\"red\")\n",
    "    plt.title(f\"{brand}: Bottom 10 Hashtags by Average Sentiment (min 50 uses)\")\n",
    "    plt.xlabel(\"Avg Compound Sentiment\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e4413",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_nopre = create_dtm(\n",
    "    list_of_strings=all_df[\"text_clean_str\"].fillna(\"\"),  # Replace NaN with empty string\n",
    "    metadata=all_df[[\"is_branded_content\", \"hashtags\", \"post_owner.name\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show first set of rows/cols\n",
    "dtm_nopre.head()\n",
    "\n",
    "## show arbitrary later cols in resulting data\n",
    "dtm_nopre.shape\n",
    "dtm_nopre.iloc[0:5, 480:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf38edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = dtm_nopre[dtm_nopre.columns[4:]].sum(axis = 0)\n",
    "\n",
    "## sorting from most frequent to least frequent\n",
    "top_terms.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea905454",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: re-tokenize and store in list\n",
    "## here, i'm doing with the raw random sample of text\n",
    "## in activity, you should do with the preprocessed texts\n",
    "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
    "                  all_df.text_clean_str]\n",
    "\n",
    "\n",
    "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
    "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
    "raw_len = len(text_raw_dict) # get length for comparison below\n",
    "\n",
    "### explore first few keys and values\n",
    "### see that key is just an arbitrary counter; value is the word itself\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
    "\n",
    "\n",
    "## Step 3: filter out very rare and very common words\n",
    "## here, i'm using the threshold that a word needs to appear in at least\n",
    "## 5% of docs but not more than 95%\n",
    "## this is an integer count of docs so i round\n",
    "lower_bound = round(all_df.shape[0]*0.05)\n",
    "upper_bound = round(all_df.shape[0]*0.95)\n",
    "\n",
    "### apply filtering to dictionary\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                             no_above = upper_bound)\n",
    "print(f'Filtering out very rare and very common words reduced the \\\n",
    "length of dictionary from {str(raw_len)} to {str(len(text_raw_dict))}.')\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]} # show first five entries after filtering\n",
    "\n",
    "## Step 4: apply dictionary to TOKENIZED texts\n",
    "## this creates a mapping between each word \n",
    "## in a specific listing and the key in the dictionary.\n",
    "## for words that remain in the filtered dictionary,\n",
    "## output is a list where len(list) == n documents\n",
    "## and each element in the list is a list of tuples\n",
    "## containing the mappings\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### can apply doc2bow(one_text, return_missing = True) to print words\n",
    "### eliminated from the listing bc they're not in filtered dictionary.\n",
    "### but feeding that one with missing values to\n",
    "### the lda function can cause errors\n",
    "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
    "                            for one_text in text_raw_tokens]\n",
    "print('Sample of documents represented in dictionary format (with omitted words noted):')\n",
    "corpus_fromdict_showmiss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3913b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: we're finally ready to estimate the model!\n",
    "## full documentation here - https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "## here, we're feeding the lda function:\n",
    "## (1) the corpus we created from the dictionary,\n",
    "## (2) a parameter we decide on for the number of topics (k),\n",
    "## (3) the dictionary itself,\n",
    "## (4) parameter for number of passes through training data (more means slower), and\n",
    "## (5) parameter that returns, for each word remaining in dict, the topic probabilities.\n",
    "## see documentation for many other arguments you can vary\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                         num_topics = 8, \n",
    "                                         id2word=text_raw_dict, \n",
    "                                         passes=6, \n",
    "                                         alpha = 'auto',\n",
    "                                         per_word_topics = True)\n",
    "\n",
    "print(type(ldamod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-model 1: explore corpus-wide summary of topics\n",
    "### getting the topics and top words; can retrieve diff top words\n",
    "topics = ldamod.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97af64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "## Post-model 2: explore topics associated with each document\n",
    "### for each item in our original dictionary, get list of topic probabilities\n",
    "l=[ldamod.get_document_topics(item) for item in corpus_fromdict]\n",
    "### print result\n",
    "text_raw_tokens[0:5]\n",
    "l[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = gensimvis.prepare(ldamod, corpus_fromdict, text_raw_dict)\n",
    "pyLDAvis.display(lda_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
